{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55605d23-41f9-4423-9f9a-831b4016479b",
   "metadata": {},
   "source": [
    "# Dropout Regularization\n",
    "\n",
    "**Dropout regularization** is a powerful technique used in deep learning to prevent overfitting in neural networks. Overfitting occurs when a model performs well on the training data but poorly on unseen or test data because it has learned to memorize the training examples rather than generalize from them. Dropout addresses this issue by randomly deactivating (or \"dropping out\") a subset of neurons during each training iteration.\n",
    "\n",
    "### How Dropout Works\n",
    "\n",
    "1. **Random Deactivation:** During training, dropout randomly sets a fraction of neurons' outputs to zero for each forward and backward pass. This means that certain neurons are temporarily removed from the network's architecture during each training iteration.\n",
    "\n",
    "2. **Variability:** By randomly deactivating neurons, dropout introduces variability into the model's learning process. This prevents any single neuron or set of neurons from becoming overly specialized to the training data, thus encouraging the network to learn more robust features.\n",
    "\n",
    "3. **Ensemble Effect:** Dropout has the effect of creating multiple subnetworks within the larger neural network. Each subnetwork is trained with different subsets of neurons. During inference (when making predictions), all neurons are active, but their outputs are scaled down by the dropout rate (typically between 0.2 and 0.5) to ensure that the overall network doesn't become overly reliant on any particular neuron.\n",
    "\n",
    "### Advantages of Dropout\n",
    "\n",
    "- **Regularization:** Dropout acts as a form of regularization, reducing the likelihood of overfitting. It encourages the network to learn more general and robust features.\n",
    "\n",
    "- **Ensemble Learning:** Dropout creates an ensemble of subnetworks during training, which can improve the model's performance. The final prediction is essentially an average or consensus of the predictions made by these subnetworks.\n",
    "\n",
    "- **Reduced Sensitivity to Noise:** Dropout makes the model less sensitive to small changes or noise in the input data, which can lead to better generalization.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "Dropout is commonly used in various types of neural networks, including convolutional neural networks (CNNs) for image classification, recurrent neural networks (RNNs) for sequential data, and fully connected networks for various tasks. It has been particularly effective in improving the performance of deep neural networks.\n",
    "\n",
    "In summary, dropout regularization is a valuable technique for improving the generalization and robustness of neural networks by randomly deactivating neurons during training. It helps prevent overfitting and enhances the model's ability to generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375b09c-57da-41d3-8a89-1c06a0d912b6",
   "metadata": {},
   "source": [
    "## ==================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6edd32c-550e-4bbe-8954-54815d1165c8",
   "metadata": {},
   "source": [
    "### Tensorflow Implementation :-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30331ae-b506-4fb9-86fa-4a541db9331f",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "modeld = keras.Sequential([\n",
    "    keras.layers.Dense(60, input_dim=60, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(15, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6719808e-6997-4212-994e-b1d5996e39f2",
   "metadata": {},
   "source": [
    "### Sequential Model\n",
    "\n",
    "The provided code snippet creates a sequential model, which is essentially a linear stack of layers. This type of model is commonly used for building feedforward neural networks where data flows sequentially through the layers.\n",
    "\n",
    "### Dense Layers\n",
    "\n",
    "The model consists of several densely connected (fully connected) layers, known as `Dense` layers. Each `Dense` layer in the model is responsible for learning and transforming the data. These layers have the following characteristics:\n",
    "\n",
    "#### Input Layer\n",
    "\n",
    "- The first `Dense` layer has 60 neurons and specifies `input_dim=60`, indicating that it expects input data with 60 features.\n",
    "- The activation function used in this layer is ReLU (Rectified Linear Unit), which introduces non-linearity into the model.\n",
    "\n",
    "#### Dropout Layers\n",
    "\n",
    "- Dropout layers are inserted between some of the `Dense` layers. These layers help prevent overfitting, a common issue in deep learning, by introducing randomness during training.\n",
    "- The dropout rates specified for these layers are 0.5, 0.1, and 0.5, respectively. These rates represent the fraction of input units (neurons) that are randomly set to zero during each training batch.\n",
    "- The first dropout layer is applied after the first `Dense` layer, the second dropout layer after the second `Dense` layer, and the third dropout layer after the third `Dense` layer.\n",
    "\n",
    "#### Output Layer\n",
    "\n",
    "- The final `Dense` layer in the model has 1 neuron.\n",
    "- It uses the sigmoid activation function, which is a common choice for binary classification problems.\n",
    "- This setup allows the model to produce a probability-like output, making it suitable for binary classification tasks.\n",
    "\n",
    "These components together form a neural network architecture with dropout regularization, aimed at improving the model's generalization and preventing overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a5d11d-ff2c-4188-9cda-2a9bc8a80d5e",
   "metadata": {},
   "source": [
    "## ==================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a0a63-dcc9-4251-a76d-f55f5ea6397b",
   "metadata": {},
   "source": [
    "\n",
    "# About Dataset\n",
    "\n",
    "The provided dataset is a tabular dataset with 61 columns and 5 rows. Each row is likely a sample or observation, while each column represents different features or attributes associated with each sample. The dataset appears to be related to a classification problem, with a binary classification label in the last column, denoted as 'R.' \n",
    "\n",
    "### Dataset Columns:\n",
    "\n",
    "- Columns 0 to 59: These columns contain numerical values that likely represent measurements or features associated with each sample. The exact meaning of these features would require additional context or domain knowledge.\n",
    "\n",
    "- Column 60 ('R'): This column serves as the target variable or label for each sample. It is binary, with 'R' likely representing one class, and the absence of 'R' representing the other class. The specific interpretation of 'R' would depend on the dataset's documentation or domain knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc2c4d5-0164-4168-aac0-742390516939",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15aa170-89ca-4221-9e2d-e84d8ac8a63f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8714fe6f-a069-44fd-94b1-8359dc35f9c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf91e9f-c609-4435-ae11-665a975aa8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122131bd-d5c5-495e-aae3-b17bfd37c96b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde15e7-a7cc-43a5-aef1-0ee4d980d886",
   "metadata": {},
   "source": [
    "### Importing Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dc7ec8b-2614-4361-aa99-f7874681b519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('sonar_dataset.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7016fcc-06ac-4b63-b2f1-147b0919f481",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.1609</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.2238</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.2999</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>0.4797</td>\n",
       "      <td>0.5783</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.4328</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.6711</td>\n",
       "      <td>0.6415</td>\n",
       "      <td>0.7104</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>0.7547</td>\n",
       "      <td>0.8537</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.6097</td>\n",
       "      <td>0.4943</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.2834</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.2641</td>\n",
       "      <td>0.1386</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.4918</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.7464</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>0.8024</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.5212</td>\n",
       "      <td>0.4052</td>\n",
       "      <td>0.3957</td>\n",
       "      <td>0.3914</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.3271</td>\n",
       "      <td>0.2767</td>\n",
       "      <td>0.4423</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.3788</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>0.1984</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>0.4182</td>\n",
       "      <td>0.3835</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.0621</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.7060</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>0.6479</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.7974</td>\n",
       "      <td>0.6737</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.3648</td>\n",
       "      <td>0.5331</td>\n",
       "      <td>0.2413</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>0.8533</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.8514</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>0.4232</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>0.6116</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>0.2129</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.1348</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.1992</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.1729</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>0.2281</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.3973</td>\n",
       "      <td>0.2741</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.5334</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>0.6260</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>0.6120</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>0.3953</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>0.5408</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.6121</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>0.3202</td>\n",
       "      <td>0.4295</td>\n",
       "      <td>0.3654</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.3952</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.4135</td>\n",
       "      <td>0.4528</td>\n",
       "      <td>0.5326</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6193</td>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.4292</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.6995</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.4724</td>\n",
       "      <td>0.5103</td>\n",
       "      <td>0.5459</td>\n",
       "      <td>0.2881</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1951</td>\n",
       "      <td>0.4181</td>\n",
       "      <td>0.4604</td>\n",
       "      <td>0.3217</td>\n",
       "      <td>0.2828</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.1979</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9       10      11      12      13      14      15      16      17  \\\n",
       "0  0.2111  0.1609  0.1582  0.2238  0.0645  0.0660  0.2273  0.3100  0.2999   \n",
       "1  0.2872  0.4918  0.6552  0.6919  0.7797  0.7464  0.9444  1.0000  0.8874   \n",
       "2  0.6194  0.6333  0.7060  0.5544  0.5320  0.6479  0.6931  0.6759  0.7551   \n",
       "3  0.1264  0.0881  0.1992  0.0184  0.2261  0.1729  0.2131  0.0693  0.2281   \n",
       "4  0.4459  0.4152  0.3952  0.4256  0.4135  0.4528  0.5326  0.7306  0.6193   \n",
       "\n",
       "       18      19      20      21      22      23      24      25      26  \\\n",
       "0  0.5078  0.4797  0.5783  0.5071  0.4328  0.5550  0.6711  0.6415  0.7104   \n",
       "1  0.8024  0.7818  0.5212  0.4052  0.3957  0.3914  0.3250  0.3200  0.3271   \n",
       "2  0.8929  0.8619  0.7974  0.6737  0.4293  0.3648  0.5331  0.2413  0.5070   \n",
       "3  0.4060  0.3973  0.2741  0.3690  0.5556  0.4846  0.3140  0.5334  0.5256   \n",
       "4  0.2032  0.4636  0.4148  0.4292  0.5730  0.5399  0.3161  0.2285  0.6995   \n",
       "\n",
       "       27      28      29      30      31      32      33      34      35  \\\n",
       "0  0.8080  0.6791  0.3857  0.1307  0.2604  0.5121  0.7547  0.8537  0.8507   \n",
       "1  0.2767  0.4423  0.2028  0.3788  0.2947  0.1984  0.2341  0.1306  0.4182   \n",
       "2  0.8533  0.6036  0.8514  0.8512  0.5045  0.1862  0.2709  0.4232  0.3043   \n",
       "3  0.2520  0.2090  0.3559  0.6260  0.7340  0.6120  0.3497  0.3953  0.3012   \n",
       "4  1.0000  0.7262  0.4724  0.5103  0.5459  0.2881  0.0981  0.1951  0.4181   \n",
       "\n",
       "       36      37      38      39      40      41      42      43      44  \\\n",
       "0  0.6692  0.6097  0.4943  0.2744  0.0510  0.2834  0.2825  0.4256  0.2641   \n",
       "1  0.3835  0.1057  0.1840  0.1970  0.1674  0.0583  0.1401  0.1628  0.0621   \n",
       "2  0.6116  0.6756  0.5375  0.4719  0.4647  0.2587  0.2129  0.2222  0.2111   \n",
       "3  0.5408  0.8814  0.9857  0.9167  0.6121  0.5006  0.3210  0.3202  0.4295   \n",
       "4  0.4604  0.3217  0.2828  0.2430  0.1979  0.2444  0.1847  0.0841  0.0692   \n",
       "\n",
       "       45      46      47      48      49      50      51      52      53  \\\n",
       "0  0.1386  0.1051  0.1343  0.0383  0.0324  0.0232  0.0027  0.0065  0.0159   \n",
       "1  0.0203  0.0530  0.0742  0.0409  0.0061  0.0125  0.0084  0.0089  0.0048   \n",
       "2  0.0176  0.1348  0.0744  0.0130  0.0106  0.0033  0.0232  0.0166  0.0095   \n",
       "3  0.3654  0.2655  0.1576  0.0681  0.0294  0.0241  0.0121  0.0036  0.0150   \n",
       "4  0.0528  0.0357  0.0085  0.0230  0.0046  0.0156  0.0031  0.0054  0.0105   \n",
       "\n",
       "       54      55      56      57      58      59 60  \n",
       "0  0.0072  0.0167  0.0180  0.0084  0.0090  0.0032  R  \n",
       "1  0.0094  0.0191  0.0140  0.0049  0.0052  0.0044  R  \n",
       "2  0.0180  0.0244  0.0316  0.0164  0.0095  0.0078  R  \n",
       "3  0.0085  0.0073  0.0050  0.0044  0.0040  0.0117  R  \n",
       "4  0.0110  0.0015  0.0072  0.0048  0.0107  0.0094  R  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2560d4b1-5093-44a6-84be-c33ac5ac6f22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d1dbe-547f-4da4-861a-09827753a778",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb2a7a9-f0ed-445a-8491-4efb2b50885d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 61 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       208 non-null    float64\n",
      " 1   1       208 non-null    float64\n",
      " 2   2       208 non-null    float64\n",
      " 3   3       208 non-null    float64\n",
      " 4   4       208 non-null    float64\n",
      " 5   5       208 non-null    float64\n",
      " 6   6       208 non-null    float64\n",
      " 7   7       208 non-null    float64\n",
      " 8   8       208 non-null    float64\n",
      " 9   9       208 non-null    float64\n",
      " 10  10      208 non-null    float64\n",
      " 11  11      208 non-null    float64\n",
      " 12  12      208 non-null    float64\n",
      " 13  13      208 non-null    float64\n",
      " 14  14      208 non-null    float64\n",
      " 15  15      208 non-null    float64\n",
      " 16  16      208 non-null    float64\n",
      " 17  17      208 non-null    float64\n",
      " 18  18      208 non-null    float64\n",
      " 19  19      208 non-null    float64\n",
      " 20  20      208 non-null    float64\n",
      " 21  21      208 non-null    float64\n",
      " 22  22      208 non-null    float64\n",
      " 23  23      208 non-null    float64\n",
      " 24  24      208 non-null    float64\n",
      " 25  25      208 non-null    float64\n",
      " 26  26      208 non-null    float64\n",
      " 27  27      208 non-null    float64\n",
      " 28  28      208 non-null    float64\n",
      " 29  29      208 non-null    float64\n",
      " 30  30      208 non-null    float64\n",
      " 31  31      208 non-null    float64\n",
      " 32  32      208 non-null    float64\n",
      " 33  33      208 non-null    float64\n",
      " 34  34      208 non-null    float64\n",
      " 35  35      208 non-null    float64\n",
      " 36  36      208 non-null    float64\n",
      " 37  37      208 non-null    float64\n",
      " 38  38      208 non-null    float64\n",
      " 39  39      208 non-null    float64\n",
      " 40  40      208 non-null    float64\n",
      " 41  41      208 non-null    float64\n",
      " 42  42      208 non-null    float64\n",
      " 43  43      208 non-null    float64\n",
      " 44  44      208 non-null    float64\n",
      " 45  45      208 non-null    float64\n",
      " 46  46      208 non-null    float64\n",
      " 47  47      208 non-null    float64\n",
      " 48  48      208 non-null    float64\n",
      " 49  49      208 non-null    float64\n",
      " 50  50      208 non-null    float64\n",
      " 51  51      208 non-null    float64\n",
      " 52  52      208 non-null    float64\n",
      " 53  53      208 non-null    float64\n",
      " 54  54      208 non-null    float64\n",
      " 55  55      208 non-null    float64\n",
      " 56  56      208 non-null    float64\n",
      " 57  57      208 non-null    float64\n",
      " 58  58      208 non-null    float64\n",
      " 59  59      208 non-null    float64\n",
      " 60  60      208 non-null    object \n",
      "dtypes: float64(60), object(1)\n",
      "memory usage: 99.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca64174-fc6e-4e40-9dd3-382645659dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>0.236013</td>\n",
       "      <td>0.250221</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.296568</td>\n",
       "      <td>0.320201</td>\n",
       "      <td>0.378487</td>\n",
       "      <td>0.415983</td>\n",
       "      <td>0.452318</td>\n",
       "      <td>0.504812</td>\n",
       "      <td>0.563047</td>\n",
       "      <td>0.609060</td>\n",
       "      <td>0.624275</td>\n",
       "      <td>0.646975</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>0.675424</td>\n",
       "      <td>0.699866</td>\n",
       "      <td>0.702155</td>\n",
       "      <td>0.694024</td>\n",
       "      <td>0.642074</td>\n",
       "      <td>0.580928</td>\n",
       "      <td>0.504475</td>\n",
       "      <td>0.439040</td>\n",
       "      <td>0.417220</td>\n",
       "      <td>0.403233</td>\n",
       "      <td>0.392571</td>\n",
       "      <td>0.384848</td>\n",
       "      <td>0.363807</td>\n",
       "      <td>0.339657</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>0.311207</td>\n",
       "      <td>0.289252</td>\n",
       "      <td>0.278293</td>\n",
       "      <td>0.246542</td>\n",
       "      <td>0.214075</td>\n",
       "      <td>0.197232</td>\n",
       "      <td>0.160631</td>\n",
       "      <td>0.122453</td>\n",
       "      <td>0.091424</td>\n",
       "      <td>0.051929</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>0.132705</td>\n",
       "      <td>0.140072</td>\n",
       "      <td>0.140962</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.205427</td>\n",
       "      <td>0.232650</td>\n",
       "      <td>0.263677</td>\n",
       "      <td>0.261529</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.262653</td>\n",
       "      <td>0.257818</td>\n",
       "      <td>0.255883</td>\n",
       "      <td>0.250175</td>\n",
       "      <td>0.239116</td>\n",
       "      <td>0.244926</td>\n",
       "      <td>0.237228</td>\n",
       "      <td>0.245657</td>\n",
       "      <td>0.237189</td>\n",
       "      <td>0.240250</td>\n",
       "      <td>0.220749</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.213237</td>\n",
       "      <td>0.206513</td>\n",
       "      <td>0.231242</td>\n",
       "      <td>0.259132</td>\n",
       "      <td>0.264121</td>\n",
       "      <td>0.239912</td>\n",
       "      <td>0.212973</td>\n",
       "      <td>0.199075</td>\n",
       "      <td>0.178662</td>\n",
       "      <td>0.171111</td>\n",
       "      <td>0.168728</td>\n",
       "      <td>0.138993</td>\n",
       "      <td>0.133291</td>\n",
       "      <td>0.151628</td>\n",
       "      <td>0.133938</td>\n",
       "      <td>0.086953</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.035954</td>\n",
       "      <td>0.013665</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.005031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>0.129250</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.166125</td>\n",
       "      <td>0.175175</td>\n",
       "      <td>0.164625</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>0.205850</td>\n",
       "      <td>0.242075</td>\n",
       "      <td>0.299075</td>\n",
       "      <td>0.350625</td>\n",
       "      <td>0.399725</td>\n",
       "      <td>0.406925</td>\n",
       "      <td>0.450225</td>\n",
       "      <td>0.540725</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.544175</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>0.534775</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.345550</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.257875</td>\n",
       "      <td>0.217575</td>\n",
       "      <td>0.179375</td>\n",
       "      <td>0.154350</td>\n",
       "      <td>0.160100</td>\n",
       "      <td>0.174275</td>\n",
       "      <td>0.173975</td>\n",
       "      <td>0.186450</td>\n",
       "      <td>0.163100</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.126875</td>\n",
       "      <td>0.094475</td>\n",
       "      <td>0.068550</td>\n",
       "      <td>0.064250</td>\n",
       "      <td>0.045125</td>\n",
       "      <td>0.026350</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.249050</td>\n",
       "      <td>0.263950</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.304700</td>\n",
       "      <td>0.308400</td>\n",
       "      <td>0.368300</td>\n",
       "      <td>0.434950</td>\n",
       "      <td>0.542500</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>0.664900</td>\n",
       "      <td>0.699700</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>0.721100</td>\n",
       "      <td>0.754500</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>0.731900</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.607150</td>\n",
       "      <td>0.490350</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.391200</td>\n",
       "      <td>0.351050</td>\n",
       "      <td>0.312750</td>\n",
       "      <td>0.321150</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.278050</td>\n",
       "      <td>0.259500</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.222550</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.121350</td>\n",
       "      <td>0.101650</td>\n",
       "      <td>0.078100</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.301650</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.351250</td>\n",
       "      <td>0.386175</td>\n",
       "      <td>0.452925</td>\n",
       "      <td>0.535725</td>\n",
       "      <td>0.659425</td>\n",
       "      <td>0.679050</td>\n",
       "      <td>0.731400</td>\n",
       "      <td>0.809325</td>\n",
       "      <td>0.816975</td>\n",
       "      <td>0.831975</td>\n",
       "      <td>0.848575</td>\n",
       "      <td>0.872175</td>\n",
       "      <td>0.873725</td>\n",
       "      <td>0.893800</td>\n",
       "      <td>0.917100</td>\n",
       "      <td>0.900275</td>\n",
       "      <td>0.852125</td>\n",
       "      <td>0.735175</td>\n",
       "      <td>0.641950</td>\n",
       "      <td>0.580300</td>\n",
       "      <td>0.556125</td>\n",
       "      <td>0.596125</td>\n",
       "      <td>0.593350</td>\n",
       "      <td>0.556525</td>\n",
       "      <td>0.518900</td>\n",
       "      <td>0.440550</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.424350</td>\n",
       "      <td>0.387525</td>\n",
       "      <td>0.384250</td>\n",
       "      <td>0.324525</td>\n",
       "      <td>0.271750</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.200375</td>\n",
       "      <td>0.154425</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.068525</td>\n",
       "      <td>0.025275</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.008525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>0.734200</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965700</td>\n",
       "      <td>0.930600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>0.773300</td>\n",
       "      <td>0.776200</td>\n",
       "      <td>0.703400</td>\n",
       "      <td>0.729200</td>\n",
       "      <td>0.552200</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \n",
       "std      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \n",
       "min      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n",
       "25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n",
       "50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n",
       "75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \n",
       "max      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.121747    0.134799    0.178003    0.208259    0.236013    0.250221   \n",
       "std      0.061788    0.085152    0.118387    0.134416    0.132705    0.140072   \n",
       "min      0.003300    0.005500    0.007500    0.011300    0.028900    0.023600   \n",
       "25%      0.080900    0.080425    0.097025    0.111275    0.129250    0.133475   \n",
       "50%      0.106950    0.112100    0.152250    0.182400    0.224800    0.249050   \n",
       "75%      0.154000    0.169600    0.233425    0.268700    0.301650    0.331250   \n",
       "max      0.372900    0.459000    0.682800    0.710600    0.734200    0.706000   \n",
       "\n",
       "               12          13          14          15          16          17  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.273305    0.296568    0.320201    0.378487    0.415983    0.452318   \n",
       "std      0.140962    0.164474    0.205427    0.232650    0.263677    0.261529   \n",
       "min      0.018400    0.027300    0.003100    0.016200    0.034900    0.037500   \n",
       "25%      0.166125    0.175175    0.164625    0.196300    0.205850    0.242075   \n",
       "50%      0.263950    0.281100    0.281700    0.304700    0.308400    0.368300   \n",
       "75%      0.351250    0.386175    0.452925    0.535725    0.659425    0.679050   \n",
       "max      0.713100    0.997000    1.000000    0.998800    1.000000    1.000000   \n",
       "\n",
       "               18          19          20          21          22          23  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.504812    0.563047    0.609060    0.624275    0.646975    0.672654   \n",
       "std      0.257988    0.262653    0.257818    0.255883    0.250175    0.239116   \n",
       "min      0.049400    0.065600    0.051200    0.021900    0.056300    0.023900   \n",
       "25%      0.299075    0.350625    0.399725    0.406925    0.450225    0.540725   \n",
       "50%      0.434950    0.542500    0.617700    0.664900    0.699700    0.698500   \n",
       "75%      0.731400    0.809325    0.816975    0.831975    0.848575    0.872175   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               24          25          26          27          28          29  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.675424    0.699866    0.702155    0.694024    0.642074    0.580928   \n",
       "std      0.244926    0.237228    0.245657    0.237189    0.240250    0.220749   \n",
       "min      0.024000    0.092100    0.048100    0.028400    0.014400    0.061300   \n",
       "25%      0.525800    0.544175    0.531900    0.534775    0.463700    0.411400   \n",
       "50%      0.721100    0.754500    0.745600    0.731900    0.680800    0.607150   \n",
       "75%      0.873725    0.893800    0.917100    0.900275    0.852125    0.735175   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               30          31          32          33          34          35  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.504475    0.439040    0.417220    0.403233    0.392571    0.384848   \n",
       "std      0.213992    0.213237    0.206513    0.231242    0.259132    0.264121   \n",
       "min      0.048200    0.040400    0.047700    0.021200    0.022300    0.008000   \n",
       "25%      0.345550    0.281400    0.257875    0.217575    0.179375    0.154350   \n",
       "50%      0.490350    0.429600    0.391200    0.351050    0.312750    0.321150   \n",
       "75%      0.641950    0.580300    0.556125    0.596125    0.593350    0.556525   \n",
       "max      0.965700    0.930600    1.000000    0.964700    1.000000    1.000000   \n",
       "\n",
       "               36          37          38          39          40          41  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.363807    0.339657    0.325800    0.311207    0.289252    0.278293   \n",
       "std      0.239912    0.212973    0.199075    0.178662    0.171111    0.168728   \n",
       "min      0.035100    0.038300    0.037100    0.011700    0.036000    0.005600   \n",
       "25%      0.160100    0.174275    0.173975    0.186450    0.163100    0.158900   \n",
       "50%      0.306300    0.312700    0.283500    0.278050    0.259500    0.245100   \n",
       "75%      0.518900    0.440550    0.434900    0.424350    0.387525    0.384250   \n",
       "max      0.949700    1.000000    0.985700    0.929700    0.899500    0.824600   \n",
       "\n",
       "               42          43          44          45          46          47  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.246542    0.214075    0.197232    0.160631    0.122453    0.091424   \n",
       "std      0.138993    0.133291    0.151628    0.133938    0.086953    0.062417   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.155200    0.126875    0.094475    0.068550    0.064250    0.045125   \n",
       "50%      0.222550    0.177700    0.148000    0.121350    0.101650    0.078100   \n",
       "75%      0.324525    0.271750    0.231550    0.200375    0.154425    0.120100   \n",
       "max      0.773300    0.776200    0.703400    0.729200    0.552200    0.333900   \n",
       "\n",
       "               48          49          50          51          52          53  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.051929    0.020424    0.016069    0.013420    0.010709    0.010941   \n",
       "std      0.035954    0.013665    0.012008    0.009634    0.007060    0.007301   \n",
       "min      0.000000    0.000000    0.000000    0.000800    0.000500    0.001000   \n",
       "25%      0.026350    0.011550    0.008425    0.007275    0.005075    0.005375   \n",
       "50%      0.044700    0.017900    0.013900    0.011400    0.009550    0.009300   \n",
       "75%      0.068525    0.025275    0.020825    0.016725    0.014900    0.014500   \n",
       "max      0.198100    0.082500    0.100400    0.070900    0.039000    0.035200   \n",
       "\n",
       "               54          55          56          57          58          59  \n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000  \n",
       "mean     0.009290    0.008222    0.007820    0.007949    0.007941    0.006507  \n",
       "std      0.007088    0.005736    0.005785    0.006470    0.006181    0.005031  \n",
       "min      0.000600    0.000400    0.000300    0.000300    0.000100    0.000600  \n",
       "25%      0.004150    0.004400    0.003700    0.003600    0.003675    0.003100  \n",
       "50%      0.007500    0.006850    0.005950    0.005800    0.006400    0.005300  \n",
       "75%      0.012100    0.010575    0.010425    0.010350    0.010325    0.008525  \n",
       "max      0.044700    0.039400    0.035500    0.044000    0.036400    0.043900  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34234ab-2eaf-4aac-b732-6c5c4dbacee9",
   "metadata": {},
   "source": [
    "### Feature and Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca34a432-577c-4e8a-a704-fad7c2e7c768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = df.drop(60,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b84ac6e4-4de9-4458-bdcb-307ec64fedf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.1609</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.2238</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.2999</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>0.4797</td>\n",
       "      <td>0.5783</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.4328</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.6711</td>\n",
       "      <td>0.6415</td>\n",
       "      <td>0.7104</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>0.7547</td>\n",
       "      <td>0.8537</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.6097</td>\n",
       "      <td>0.4943</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.2834</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.2641</td>\n",
       "      <td>0.1386</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.4918</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.7464</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>0.8024</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.5212</td>\n",
       "      <td>0.4052</td>\n",
       "      <td>0.3957</td>\n",
       "      <td>0.3914</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.3271</td>\n",
       "      <td>0.2767</td>\n",
       "      <td>0.4423</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.3788</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>0.1984</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>0.4182</td>\n",
       "      <td>0.3835</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.0621</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.7060</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>0.6479</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.7974</td>\n",
       "      <td>0.6737</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.3648</td>\n",
       "      <td>0.5331</td>\n",
       "      <td>0.2413</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>0.8533</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.8514</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>0.4232</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>0.6116</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>0.2129</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.1348</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.1992</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.1729</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>0.2281</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.3973</td>\n",
       "      <td>0.2741</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.5334</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>0.6260</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>0.6120</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>0.3953</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>0.5408</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.6121</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>0.3202</td>\n",
       "      <td>0.4295</td>\n",
       "      <td>0.3654</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.3952</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.4135</td>\n",
       "      <td>0.4528</td>\n",
       "      <td>0.5326</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6193</td>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.4292</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.6995</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.4724</td>\n",
       "      <td>0.5103</td>\n",
       "      <td>0.5459</td>\n",
       "      <td>0.2881</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1951</td>\n",
       "      <td>0.4181</td>\n",
       "      <td>0.4604</td>\n",
       "      <td>0.3217</td>\n",
       "      <td>0.2828</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.1979</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>0.3108</td>\n",
       "      <td>0.2933</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>0.0994</td>\n",
       "      <td>0.1801</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2034</td>\n",
       "      <td>0.1740</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>0.6879</td>\n",
       "      <td>0.8120</td>\n",
       "      <td>0.8453</td>\n",
       "      <td>0.8919</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8104</td>\n",
       "      <td>0.6199</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>0.4160</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0969</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.1676</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>0.1977</td>\n",
       "      <td>0.1339</td>\n",
       "      <td>0.0902</td>\n",
       "      <td>0.1085</td>\n",
       "      <td>0.1521</td>\n",
       "      <td>0.1363</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>0.3085</td>\n",
       "      <td>0.3425</td>\n",
       "      <td>0.2990</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>0.1534</td>\n",
       "      <td>0.1901</td>\n",
       "      <td>0.2429</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.3272</td>\n",
       "      <td>0.5949</td>\n",
       "      <td>0.8302</td>\n",
       "      <td>0.9045</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9092</td>\n",
       "      <td>0.7412</td>\n",
       "      <td>0.7691</td>\n",
       "      <td>0.7117</td>\n",
       "      <td>0.5304</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>0.1297</td>\n",
       "      <td>0.1159</td>\n",
       "      <td>0.1226</td>\n",
       "      <td>0.1768</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.1149</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0790</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>0.2716</td>\n",
       "      <td>0.2374</td>\n",
       "      <td>0.1878</td>\n",
       "      <td>0.0983</td>\n",
       "      <td>0.0683</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.1723</td>\n",
       "      <td>0.2339</td>\n",
       "      <td>0.1962</td>\n",
       "      <td>0.1395</td>\n",
       "      <td>0.3164</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>0.7631</td>\n",
       "      <td>0.8473</td>\n",
       "      <td>0.9424</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9699</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8630</td>\n",
       "      <td>0.6979</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.7305</td>\n",
       "      <td>0.5197</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1446</td>\n",
       "      <td>0.1066</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.1929</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>0.0910</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>0.1059</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>0.0535</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.2898</td>\n",
       "      <td>0.2812</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.0673</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.2070</td>\n",
       "      <td>0.2645</td>\n",
       "      <td>0.2828</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.5685</td>\n",
       "      <td>0.6990</td>\n",
       "      <td>0.7246</td>\n",
       "      <td>0.7622</td>\n",
       "      <td>0.9242</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.8297</td>\n",
       "      <td>0.7032</td>\n",
       "      <td>0.7141</td>\n",
       "      <td>0.6893</td>\n",
       "      <td>0.4961</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.0969</td>\n",
       "      <td>0.0776</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.1572</td>\n",
       "      <td>0.1823</td>\n",
       "      <td>0.1349</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.1552</td>\n",
       "      <td>0.1548</td>\n",
       "      <td>0.1319</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>0.2442</td>\n",
       "      <td>0.1665</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.1302</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>0.2177</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.4552</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.7397</td>\n",
       "      <td>0.8062</td>\n",
       "      <td>0.8837</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.7603</td>\n",
       "      <td>0.7123</td>\n",
       "      <td>0.8358</td>\n",
       "      <td>0.7622</td>\n",
       "      <td>0.4567</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.1549</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.1869</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.1713</td>\n",
       "      <td>0.0959</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.0847</td>\n",
       "      <td>0.2076</td>\n",
       "      <td>0.2505</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.1439</td>\n",
       "      <td>0.1470</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows  60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "         9       10      11      12      13      14      15      16      17  \\\n",
       "0    0.2111  0.1609  0.1582  0.2238  0.0645  0.0660  0.2273  0.3100  0.2999   \n",
       "1    0.2872  0.4918  0.6552  0.6919  0.7797  0.7464  0.9444  1.0000  0.8874   \n",
       "2    0.6194  0.6333  0.7060  0.5544  0.5320  0.6479  0.6931  0.6759  0.7551   \n",
       "3    0.1264  0.0881  0.1992  0.0184  0.2261  0.1729  0.2131  0.0693  0.2281   \n",
       "4    0.4459  0.4152  0.3952  0.4256  0.4135  0.4528  0.5326  0.7306  0.6193   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.2684  0.3108  0.2933  0.2275  0.0994  0.1801  0.2200  0.2732  0.2862   \n",
       "204  0.2154  0.3085  0.3425  0.2990  0.1402  0.1235  0.1534  0.1901  0.2429   \n",
       "205  0.2529  0.2716  0.2374  0.1878  0.0983  0.0683  0.1503  0.1723  0.2339   \n",
       "206  0.2354  0.2898  0.2812  0.1578  0.0273  0.0673  0.1444  0.2070  0.2645   \n",
       "207  0.2354  0.2720  0.2442  0.1665  0.0336  0.1302  0.1708  0.2177  0.3175   \n",
       "\n",
       "         18      19      20      21      22      23      24      25      26  \\\n",
       "0    0.5078  0.4797  0.5783  0.5071  0.4328  0.5550  0.6711  0.6415  0.7104   \n",
       "1    0.8024  0.7818  0.5212  0.4052  0.3957  0.3914  0.3250  0.3200  0.3271   \n",
       "2    0.8929  0.8619  0.7974  0.6737  0.4293  0.3648  0.5331  0.2413  0.5070   \n",
       "3    0.4060  0.3973  0.2741  0.3690  0.5556  0.4846  0.3140  0.5334  0.5256   \n",
       "4    0.2032  0.4636  0.4148  0.4292  0.5730  0.5399  0.3161  0.2285  0.6995   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.2034  0.1740  0.4130  0.6879  0.8120  0.8453  0.8919  0.9300  0.9987   \n",
       "204  0.2120  0.2395  0.3272  0.5949  0.8302  0.9045  0.9888  0.9912  0.9448   \n",
       "205  0.1962  0.1395  0.3164  0.5888  0.7631  0.8473  0.9424  0.9986  0.9699   \n",
       "206  0.2828  0.4293  0.5685  0.6990  0.7246  0.7622  0.9242  1.0000  0.9979   \n",
       "207  0.3714  0.4552  0.5700  0.7397  0.8062  0.8837  0.9432  1.0000  0.9375   \n",
       "\n",
       "         27      28      29      30      31      32      33      34      35  \\\n",
       "0    0.8080  0.6791  0.3857  0.1307  0.2604  0.5121  0.7547  0.8537  0.8507   \n",
       "1    0.2767  0.4423  0.2028  0.3788  0.2947  0.1984  0.2341  0.1306  0.4182   \n",
       "2    0.8533  0.6036  0.8514  0.8512  0.5045  0.1862  0.2709  0.4232  0.3043   \n",
       "3    0.2520  0.2090  0.3559  0.6260  0.7340  0.6120  0.3497  0.3953  0.3012   \n",
       "4    1.0000  0.7262  0.4724  0.5103  0.5459  0.2881  0.0981  0.1951  0.4181   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  1.0000  0.8104  0.6199  0.6041  0.5547  0.4160  0.1472  0.0849  0.0608   \n",
       "204  1.0000  0.9092  0.7412  0.7691  0.7117  0.5304  0.2131  0.0928  0.1297   \n",
       "205  1.0000  0.8630  0.6979  0.7717  0.7305  0.5197  0.1786  0.1098  0.1446   \n",
       "206  0.8297  0.7032  0.7141  0.6893  0.4961  0.2584  0.0969  0.0776  0.0364   \n",
       "207  0.7603  0.7123  0.8358  0.7622  0.4567  0.1715  0.1549  0.1641  0.1869   \n",
       "\n",
       "         36      37      38      39      40      41      42      43      44  \\\n",
       "0    0.6692  0.6097  0.4943  0.2744  0.0510  0.2834  0.2825  0.4256  0.2641   \n",
       "1    0.3835  0.1057  0.1840  0.1970  0.1674  0.0583  0.1401  0.1628  0.0621   \n",
       "2    0.6116  0.6756  0.5375  0.4719  0.4647  0.2587  0.2129  0.2222  0.2111   \n",
       "3    0.5408  0.8814  0.9857  0.9167  0.6121  0.5006  0.3210  0.3202  0.4295   \n",
       "4    0.4604  0.3217  0.2828  0.2430  0.1979  0.2444  0.1847  0.0841  0.0692   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0969  0.1411  0.1676  0.1200  0.1201  0.1036  0.1977  0.1339  0.0902   \n",
       "204  0.1159  0.1226  0.1768  0.0345  0.1562  0.0824  0.1149  0.1694  0.0954   \n",
       "205  0.1066  0.1440  0.1929  0.0325  0.1490  0.0328  0.0537  0.1309  0.0910   \n",
       "206  0.1572  0.1823  0.1349  0.0849  0.0492  0.1367  0.1552  0.1548  0.1319   \n",
       "207  0.2655  0.1713  0.0959  0.0768  0.0847  0.2076  0.2505  0.1862  0.1439   \n",
       "\n",
       "         45      46      47      48      49      50      51      52      53  \\\n",
       "0    0.1386  0.1051  0.1343  0.0383  0.0324  0.0232  0.0027  0.0065  0.0159   \n",
       "1    0.0203  0.0530  0.0742  0.0409  0.0061  0.0125  0.0084  0.0089  0.0048   \n",
       "2    0.0176  0.1348  0.0744  0.0130  0.0106  0.0033  0.0232  0.0166  0.0095   \n",
       "3    0.3654  0.2655  0.1576  0.0681  0.0294  0.0241  0.0121  0.0036  0.0150   \n",
       "4    0.0528  0.0357  0.0085  0.0230  0.0046  0.0156  0.0031  0.0054  0.0105   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.1085  0.1521  0.1363  0.0858  0.0290  0.0203  0.0116  0.0098  0.0199   \n",
       "204  0.0080  0.0790  0.1255  0.0647  0.0179  0.0051  0.0061  0.0093  0.0135   \n",
       "205  0.0757  0.1059  0.1005  0.0535  0.0235  0.0155  0.0160  0.0029  0.0051   \n",
       "206  0.0985  0.1258  0.0954  0.0489  0.0241  0.0042  0.0086  0.0046  0.0126   \n",
       "207  0.1470  0.0991  0.0041  0.0154  0.0116  0.0181  0.0146  0.0129  0.0047   \n",
       "\n",
       "         54      55      56      57      58      59  \n",
       "0    0.0072  0.0167  0.0180  0.0084  0.0090  0.0032  \n",
       "1    0.0094  0.0191  0.0140  0.0049  0.0052  0.0044  \n",
       "2    0.0180  0.0244  0.0316  0.0164  0.0095  0.0078  \n",
       "3    0.0085  0.0073  0.0050  0.0044  0.0040  0.0117  \n",
       "4    0.0110  0.0015  0.0072  0.0048  0.0107  0.0094  \n",
       "..      ...     ...     ...     ...     ...     ...  \n",
       "203  0.0033  0.0101  0.0065  0.0115  0.0193  0.0157  \n",
       "204  0.0063  0.0063  0.0034  0.0032  0.0062  0.0067  \n",
       "205  0.0062  0.0089  0.0140  0.0138  0.0077  0.0031  \n",
       "206  0.0036  0.0035  0.0034  0.0079  0.0036  0.0048  \n",
       "207  0.0039  0.0061  0.0040  0.0036  0.0061  0.0115  \n",
       "\n",
       "[208 rows x 60 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72c849b4-120f-4f9f-ba00-2289935fc56c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = df[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d44eb248-63f1-43b1-bfda-2e8abd613b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f17152b-219b-4a30-94d3-a863c143b735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     R\n",
       "207  0\n",
       "152  0\n",
       "34   1\n",
       "167  0\n",
       "46   1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.get_dummies(y,drop_first=True)\n",
    "y.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a293b01-3574-45ec-9b93-50071c5863d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R\n",
       "0    111\n",
       "1     97\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bdd2f-448b-4550-a171-1170de3b7a28",
   "metadata": {},
   "source": [
    "### Test train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "942b31da-549b-4118-84f3-42b628114aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.3,random_state=88,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54f0b7e3-cbd7-4850-9d57-1e5247cd9865",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((145, 60), (63, 60), (145, 1), (63, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape,xtest.shape,ytrain.shape,ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d8b32-fc41-4ee8-b911-0d69c6d9a7f9",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9d83625-fb35-4a4e-aaf6-cfe05896ebb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.6864 - accuracy: 0.5241\n",
      "Epoch 2/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6645 - accuracy: 0.6966\n",
      "Epoch 3/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6447 - accuracy: 0.6414\n",
      "Epoch 4/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6252 - accuracy: 0.7379\n",
      "Epoch 5/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6020 - accuracy: 0.6690\n",
      "Epoch 6/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5722 - accuracy: 0.7931\n",
      "Epoch 7/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5458 - accuracy: 0.7655\n",
      "Epoch 8/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5181 - accuracy: 0.7517\n",
      "Epoch 9/70\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5031 - accuracy: 0.7655\n",
      "Epoch 10/70\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4709 - accuracy: 0.8207\n",
      "Epoch 11/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.8276\n",
      "Epoch 12/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4377 - accuracy: 0.8069\n",
      "Epoch 13/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4132 - accuracy: 0.8276\n",
      "Epoch 14/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.8207\n",
      "Epoch 15/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3926 - accuracy: 0.8552\n",
      "Epoch 16/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3740 - accuracy: 0.8690\n",
      "Epoch 17/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3575 - accuracy: 0.8621\n",
      "Epoch 18/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3294 - accuracy: 0.8621\n",
      "Epoch 19/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3256 - accuracy: 0.8828\n",
      "Epoch 20/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3083 - accuracy: 0.8828\n",
      "Epoch 21/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3151 - accuracy: 0.8621\n",
      "Epoch 22/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3566 - accuracy: 0.8069\n",
      "Epoch 23/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3719 - accuracy: 0.8069\n",
      "Epoch 24/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2885 - accuracy: 0.8966\n",
      "Epoch 25/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.8759\n",
      "Epoch 26/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2699 - accuracy: 0.8828\n",
      "Epoch 27/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2531 - accuracy: 0.9034\n",
      "Epoch 28/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2308 - accuracy: 0.9172\n",
      "Epoch 29/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.8690\n",
      "Epoch 30/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2359 - accuracy: 0.9172\n",
      "Epoch 31/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2319 - accuracy: 0.9103\n",
      "Epoch 32/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2184 - accuracy: 0.9103\n",
      "Epoch 33/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2362 - accuracy: 0.9103\n",
      "Epoch 34/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2205 - accuracy: 0.9172\n",
      "Epoch 35/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2353 - accuracy: 0.8828\n",
      "Epoch 36/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1921 - accuracy: 0.9241\n",
      "Epoch 37/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9448\n",
      "Epoch 38/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9241\n",
      "Epoch 39/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9379\n",
      "Epoch 40/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1534 - accuracy: 0.9379\n",
      "Epoch 41/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1557 - accuracy: 0.9379\n",
      "Epoch 42/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2083 - accuracy: 0.8966\n",
      "Epoch 43/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9517\n",
      "Epoch 44/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9586\n",
      "Epoch 45/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.1609 - accuracy: 0.9379\n",
      "Epoch 46/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.9517\n",
      "Epoch 47/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.1367 - accuracy: 0.9448\n",
      "Epoch 48/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.1552 - accuracy: 0.9379\n",
      "Epoch 49/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.1164 - accuracy: 0.9655\n",
      "Epoch 50/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.1046 - accuracy: 0.9724\n",
      "Epoch 51/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9793\n",
      "Epoch 52/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.1103 - accuracy: 0.9862\n",
      "Epoch 53/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0952 - accuracy: 0.9793\n",
      "Epoch 54/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0765 - accuracy: 0.9931\n",
      "Epoch 55/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9931\n",
      "Epoch 56/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0725 - accuracy: 0.9793\n",
      "Epoch 57/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0700 - accuracy: 0.9862\n",
      "Epoch 58/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0625 - accuracy: 0.9862\n",
      "Epoch 59/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.9931\n",
      "Epoch 60/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0532 - accuracy: 0.9931\n",
      "Epoch 61/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.9931\n",
      "Epoch 62/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0537 - accuracy: 0.9931\n",
      "Epoch 63/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0465 - accuracy: 1.0000\n",
      "Epoch 64/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0434 - accuracy: 0.9931\n",
      "Epoch 65/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0387 - accuracy: 1.0000\n",
      "Epoch 66/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0392 - accuracy: 0.9931\n",
      "Epoch 67/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0384 - accuracy: 1.0000\n",
      "Epoch 68/70\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0451 - accuracy: 1.0000\n",
      "Epoch 69/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0455 - accuracy: 0.9862\n",
      "Epoch 70/70\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0306 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x148c199a610>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(60, input_dim=60, activation='relu'),\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(15, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain, ytrain, epochs=70, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb191afe-88ef-402a-841c-bb800650a051",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 0.4137 - accuracy: 0.8254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.41367533802986145, 0.8253968358039856]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939815b-f6ba-4d42-9c1f-bb02188712b6",
   "metadata": {},
   "source": [
    "#### We can see the model has accuracy of 100% on training data but has only 82% accuracy on test data so, our model overfits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ba188-c7fd-46e5-9d47-038b1c4d8a41",
   "metadata": {},
   "source": [
    "### Model with Droping out some random neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf0ebff1-29bf-4a1c-8f20-f956d9c9b77a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.7337 - accuracy: 0.4690\n",
      "Epoch 2/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6852 - accuracy: 0.5586\n",
      "Epoch 3/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5655\n",
      "Epoch 4/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6719 - accuracy: 0.6414\n",
      "Epoch 5/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.7026 - accuracy: 0.4897\n",
      "Epoch 6/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.6689 - accuracy: 0.5724\n",
      "Epoch 7/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6638 - accuracy: 0.5931\n",
      "Epoch 8/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6815 - accuracy: 0.6069\n",
      "Epoch 9/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6958 - accuracy: 0.5241\n",
      "Epoch 10/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6612 - accuracy: 0.6276\n",
      "Epoch 11/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6486 - accuracy: 0.6414\n",
      "Epoch 12/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6492 - accuracy: 0.6000\n",
      "Epoch 13/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6626 - accuracy: 0.6207\n",
      "Epoch 14/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6736 - accuracy: 0.5655\n",
      "Epoch 15/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6385 - accuracy: 0.6828\n",
      "Epoch 16/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6431 - accuracy: 0.6690\n",
      "Epoch 17/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6184 - accuracy: 0.6966\n",
      "Epoch 18/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6260 - accuracy: 0.6966\n",
      "Epoch 19/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6192 - accuracy: 0.6759\n",
      "Epoch 20/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6202 - accuracy: 0.6552\n",
      "Epoch 21/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5854 - accuracy: 0.7103\n",
      "Epoch 22/100\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6319 - accuracy: 0.6552\n",
      "Epoch 23/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.6073 - accuracy: 0.6897\n",
      "Epoch 24/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6323 - accuracy: 0.6759\n",
      "Epoch 25/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5920 - accuracy: 0.7172\n",
      "Epoch 26/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6042 - accuracy: 0.6759\n",
      "Epoch 27/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.7103\n",
      "Epoch 28/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5787 - accuracy: 0.7172\n",
      "Epoch 29/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5409 - accuracy: 0.7034\n",
      "Epoch 30/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7517\n",
      "Epoch 31/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5567 - accuracy: 0.7103\n",
      "Epoch 32/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5139 - accuracy: 0.7793\n",
      "Epoch 33/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5103 - accuracy: 0.7241\n",
      "Epoch 34/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7793\n",
      "Epoch 35/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5212 - accuracy: 0.7103\n",
      "Epoch 36/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5211 - accuracy: 0.7655\n",
      "Epoch 37/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5078 - accuracy: 0.7517\n",
      "Epoch 38/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4954 - accuracy: 0.7724\n",
      "Epoch 39/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5353 - accuracy: 0.7655\n",
      "Epoch 40/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4499 - accuracy: 0.8069\n",
      "Epoch 41/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7793\n",
      "Epoch 42/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4995 - accuracy: 0.7379\n",
      "Epoch 43/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4338 - accuracy: 0.8000\n",
      "Epoch 44/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4723 - accuracy: 0.8000\n",
      "Epoch 45/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4650 - accuracy: 0.8276\n",
      "Epoch 46/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4518 - accuracy: 0.8138\n",
      "Epoch 47/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8207\n",
      "Epoch 48/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8621\n",
      "Epoch 49/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.8138\n",
      "Epoch 50/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.7655\n",
      "Epoch 51/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4173 - accuracy: 0.8207\n",
      "Epoch 52/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8414\n",
      "Epoch 53/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8276\n",
      "Epoch 54/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3993 - accuracy: 0.8414\n",
      "Epoch 55/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4538 - accuracy: 0.8069\n",
      "Epoch 56/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3988 - accuracy: 0.8345\n",
      "Epoch 57/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3840 - accuracy: 0.8276\n",
      "Epoch 58/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8138\n",
      "Epoch 59/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.8483\n",
      "Epoch 60/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4860 - accuracy: 0.7931\n",
      "Epoch 61/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8414\n",
      "Epoch 62/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.8276\n",
      "Epoch 63/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8207\n",
      "Epoch 64/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3774 - accuracy: 0.8483\n",
      "Epoch 65/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3784 - accuracy: 0.8345\n",
      "Epoch 66/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4540 - accuracy: 0.8207\n",
      "Epoch 67/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3557 - accuracy: 0.8207\n",
      "Epoch 68/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4119 - accuracy: 0.8207\n",
      "Epoch 69/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3702 - accuracy: 0.8621\n",
      "Epoch 70/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3756 - accuracy: 0.8483\n",
      "Epoch 71/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4254 - accuracy: 0.8483\n",
      "Epoch 72/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3864 - accuracy: 0.8759\n",
      "Epoch 73/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3503 - accuracy: 0.8690\n",
      "Epoch 74/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8552\n",
      "Epoch 75/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3569 - accuracy: 0.8552\n",
      "Epoch 76/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.4071 - accuracy: 0.8138\n",
      "Epoch 77/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3307 - accuracy: 0.8897\n",
      "Epoch 78/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3575 - accuracy: 0.8483\n",
      "Epoch 79/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3137 - accuracy: 0.8828\n",
      "Epoch 80/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3419 - accuracy: 0.8621\n",
      "Epoch 81/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8552\n",
      "Epoch 82/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3593 - accuracy: 0.8276\n",
      "Epoch 83/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3392 - accuracy: 0.8759\n",
      "Epoch 84/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3577 - accuracy: 0.8483\n",
      "Epoch 85/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8759\n",
      "Epoch 86/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3472 - accuracy: 0.8690\n",
      "Epoch 87/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3528 - accuracy: 0.8828\n",
      "Epoch 88/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2987 - accuracy: 0.8966\n",
      "Epoch 89/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2872 - accuracy: 0.9241\n",
      "Epoch 90/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3586 - accuracy: 0.8690\n",
      "Epoch 91/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2873 - accuracy: 0.9034\n",
      "Epoch 92/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3447 - accuracy: 0.8483\n",
      "Epoch 93/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2634 - accuracy: 0.8966\n",
      "Epoch 94/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2542 - accuracy: 0.9310\n",
      "Epoch 95/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2599 - accuracy: 0.9034\n",
      "Epoch 96/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.8828\n",
      "Epoch 97/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3006 - accuracy: 0.8897\n",
      "Epoch 98/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3082 - accuracy: 0.9034\n",
      "Epoch 99/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2549 - accuracy: 0.9034\n",
      "Epoch 100/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.2829 - accuracy: 0.8897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x148c1d63810>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeld = keras.Sequential([\n",
    "    keras.layers.Dense(60, input_dim=60, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(15, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "modeld.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "modeld.fit(xtrain, ytrain, epochs=100, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bca1719e-fbaa-4117-b50e-378b2f9976b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 0.3337 - accuracy: 0.8413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33374807238578796, 0.841269850730896]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeld.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623da08a-637a-406b-bb7d-d1e0c5f315d7",
   "metadata": {},
   "source": [
    "#### We can see this is a good model because the variance between the train and test accuracy is not too much"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
